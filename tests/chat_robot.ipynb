{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "load_dotenv()\n",
    "# 初始化模型：deepseek-r1:32b ｜ qwq:32b\n",
    "chat_model = ChatOllama(\n",
    "    base_url=os.environ['OLLAMA_BASE_URL'],  # 可配置为内部服务器地址\n",
    "    model=\"deepseek-r1:32b\",\n",
    "    temperature=0.8,\n",
    "    num_ctx=4096,  # 上下文窗口大小\n",
    "    streaming=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 定义提示模板（含历史消息占位符）\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个专业的助手，用中文简洁回答问题。\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 3. 构建链式调用\n",
    "chain = prompt | chat_model\n",
    "\n",
    "# 4. 添加对话历史管理\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入'exit'结束对话\n",
      "\n",
      "<think>\n",
      "好的，我现在要描述LangChain这个框架。首先，我记得它是一个开源的工具包，主要用于构建AI应用，尤其是那些需要处理复杂任务的应用。\n",
      "\n",
      "接下来，我应该提到它的核心功能，比如知识库管理和链式操作。知识库管理包括文档加载、分块和向量化，这些都是构建智能系统的基础步骤。而链式操作允许用户将多个LLM调用组合起来，形成更复杂的逻辑流程。\n",
      "\n",
      "然后，LangChain支持多种主流模型，如GPT系列和Claude，这为开发者提供了灵活的选择空间。同时，它还提供了一系列示例和工具，帮助用户快速上手，比如问答、总结和聊天机器人等场景的实现。\n",
      "\n",
      "最后，我要强调LangChain的优势在于其模块化设计，使得开发者可以轻松构建和扩展自己的AI应用，适用于各种实际场景。这样，整个描述既全面又简洁，能够清晰传达LangChain的功能和特点。\n",
      "</think>\n",
      "\n",
      "LangChain 是一个开源工具包，用于构建 AI 应用程序。它支持知识库管理和链式操作（将多个 LLM 调用组合成复杂逻辑），并兼容多种主流模型。通过丰富的示例和工具，如问答、总结等场景实现，LangChain 帮助开发者快速构建高效且灵活的 AI 解决方案。<think>\n",
      "好，现在要详细说明LangChain的核心功能。\n",
      "\n",
      "首先，知识库管理是关键部分。这里包括文档加载器、文本分块器和向量化工具。这些工具帮助用户将各种格式的文档转化为适合LLM处理的数据形式，并进行高效的检索和存储。\n",
      "\n",
      "其次，链式操作是另一个核心点。通过组合多个LLM调用，用户可以构建复杂的逻辑流程，比如先提取关键信息，再生成最终输出。这使得应用更加灵活和智能化。\n",
      "\n",
      "然后，模型支持也是LangChain的重要功能之一。它兼容GPT-3/4、Claude等主流模型，并允许开发者自定义提示模板，增强了应用的可定制性。\n",
      "\n",
      "此外，工具集的丰富性是其一大亮点，包括问答、总结、聊天机器人等功能模块，为常见应用场景提供了便捷的解决方案。\n",
      "\n",
      "最后，社区支持和文档完善也是LangChain的优势所在。活跃的社区和详细的文档帮助用户快速上手和解决问题，推动了项目的广泛应用和发展。\n",
      "</think>\n",
      "\n",
      "LangChain 的核心功能主要包括以下几点：\n",
      "\n",
      "1. **知识库管理**：\n",
      "   - 文档加载器：支持从多种来源（如本地文件、URL、数据库等）加载文档。\n",
      "   - 文本分块器：将大段文本分割成适合处理的小块，便于后续分析和检索。\n",
      "   - 向量化工具：将文本转化为向量表示，以便进行高效的相似性搜索。\n",
      "\n",
      "2. **链式操作**：\n",
      "   - 允许用户组合多个LLM调用，构建复杂的逻辑流程。例如，先使用一个模型提取关键信息，再用另一个模型生成最终输出。\n",
      "\n",
      "3. **模型支持**：\n",
      "   - 支持多种主流模型（如GPT-3/4、Claude等），并提供自定义提示模板的能力，增强应用的灵活性和可定制性。\n",
      "\n",
      "4. **工具集**：\n",
      "   - 提供丰富的工具模块，涵盖问答系统、文本摘要、聊天机器人等多种应用场景，帮助用户快速实现功能。\n",
      "\n",
      "5. **社区与文档支持**：\n",
      "   - 拥有活跃的开源社区，提供大量示例代码和详细文档，便于开发者快速上手和解决问题。\n",
      "\n",
      "这些功能使LangChain成为一个强大且灵活的工具包，广泛应用于构建智能问答、文档分析、自动化流程等多种AI应用场景。"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 命令行交互逻辑\n",
    "def chat_interface():\n",
    "    session_id = input(\"请输入会话ID（例如user123）：\")\n",
    "    print(\"输入'exit'结束对话\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"用户：\")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = chain_with_history.stream(\n",
    "            {\"input\": user_input},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        for chunk in response:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "chat_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
